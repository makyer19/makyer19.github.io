---
title: Clustering
author: Michael Alex Kyer
date: 11/06/2023
format:
  html:
    code-fold: true
jupyter: python3
---

# What is Clustering
Clustering is a method of unsupervised machine learning. The goal is to group, or cluster, sets of data points within a distribution. Based on these different clusters, information can be inferred as to what it means to be in one group or another.

### Clustering Compared to Classification
At a first glance, clustering and classification may appear to be quite similar. The difference lies in that while classification tends to have predefined sets of categories that points will be placed into, clustering instead groups points by similarities. Therefore, as classification uses predefined labels, it is typically used in supervised learning. On the other hand, clustering data is not labeled, and so it is used during unsupervised learning.

### Types of Clustering
There are multiple different methods of clustering.

Centroid based clustering is a common clustering technique that allocates a center to each cluster. Each point is measured from a distance to each center, and it is placed in the cluster whose center it is closest to. We will be using the K-means algorithm for our clustering example later, and we will explain what that is in the next section.

Density based clustering places points of high density into randomly constructed distributions. To further utilize density, no outliers are observed or placed within the clusters.

Distribution based clustering takes input data of distributions and takes the distance from each point to the center of each other distribution. The probability that a point is within a certain distribution is lower the further it is from a distributions center.

Hierarchical clustering involves creating a tree of clusters based on hierarchical data. This often leads to clusters being placed within other clusters.

# The K-means Algorithm
As stated previously, the K-means algorithm is a centroid based clustering algorithm that separates the points into K different clusters each with a center. In this case, the centroids are determined at random, and each centroid is chosen from the list of points given. Therefore, the biggest variable here is the number, K, clusters we are looking for. The algorithm can be optimized easily by repeatedly changing K until the sum of the squared distance between each point and the closest centroid is minimized. After randomly selecting the centroids, each point is placed into the cluster of its nearest centroid. From there, the actual center of each cluster is calculated by averaging coordinate values, and those centers become the new centroids. Finally, each point is again placed into the cluster that the nearest centroid is assigned to.

# Clustering Example: Grouping the Penguins
Similarly to some previous examples, we will once again use the penguins data set. At face value, each penguin seems to have a predefined label in either its island or species. However, we can easily ignore these values in order to utilize an unsupervised, clustering approach. Like always, we will begin with some pre-processing.

```{python}
#| tags: []
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans

penguins_data = pd.read_csv('penguins_size.csv')
penguins_data = penguins_data.dropna()

penguins_data = penguins_data.drop('species', axis=1)
penguins_data = penguins_data.drop('island', axis=1)
penguins_data = penguins_data.drop('sex', axis=1)
```

### Using K-means to Cluster the Data
Next, we will cluster the data based off two variables of our numerical data. Those input data being culmen length, culmen depth, body mass, and flipper length. Recalling from our previous probability distribution blog post, in order to differentiate between Adelie and Chinstrap penguins, we should have one of the categories be culmen length. This might seem like it falls into the classification category, but this new set of data does not have any classifier information. We'll set our number of clusters to 3. This can be done with some optimization, but we already know from previous experience that we have 3 species of penguin in or dataset, so we can skip that step here. Now we can use our K-means algorithm which works well for two reasons. First, K-means is a very fast algorithm that executes in O(n) time where n is the number of points given. Secondly, this can be done for a very large dataset in which there is not enough time to place each penguin into a category of species or island. We can then create multiple plots which cluster our data based on two variables in each way possible, but we will just display the cluster on culmen length and flipper length. This will help to avoid confusion in our analysis.

```{python}
#| tags: []
X = penguins_data[['flipper_length_mm','culmen_length_mm']]

kmeans = KMeans(n_clusters=3, n_init=10)
kmeans.fit(X)
y_kmeans = kmeans.predict(X)

plt.scatter(X.loc[:, 'flipper_length_mm'], X.loc[:, 'culmen_length_mm'], c=y_kmeans, s=50, cmap='viridis')
centers = kmeans.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)
plt.xlabel('Flipper Length (mm)')
plt.ylabel('Culmen Length (mm)')
plt.title('Clustering on Culmen Length and Flipper Length')
plt.show()
```

### Analyzing the Results
Observing the cluster scatter plot shown above, we can clearly see the 3 different clusters. As the colors may change, I will refer to them as bottom left, middle, and upper right. As we do not have any classifiers, we are not positive as to what the clusters truly mean. They may refer to species or island, but we cannot be immediately sure. What we do know, is that the bottom left penguins have short culmen and flippers, the upper left have long culmen and flippers, and the middle have medium length flippers with longer culmen. From that we can infer that each grouping of penguin may have something in common. Indeed, the bottom left is majorly comprised of Adelie penguins, the middle of Chinstrap, and the upper left of Gentoo. Without that information though, we can begin making predictions, or using that data to our advantage. Maybe certain penguins are vulnerable to an illness, and a good identifier are penguins with short flippers and culmen. We don't know the type of those penguins, but we can still identify the penguins within that cluster as potentially at risk. This is just one example of how clustering can be used to gather useful information for data without classifiers.

# Conclusion
With all this in mind, we can see how clustering can be utilized in machine learning to make predictions or analyze data that do not have any specified clusters. Whether the data set is too large to label each point with a classifier, or there are no classifiers available, we can still analyze the data in a useful way. I hope you were able to learn some information about clustering from this blogpost. If you would like to learn more, you can visit these resources that I used in the formulation of this blogpost. 

Pengiun Dataset - (https://www.kaggle.com/datasets/parulpandey/palmer-archipelago-antarctica-penguin-data/) \
explorium: Clustering â€” When You Should Use it and Avoid It (https://www.explorium.ai/blog/machine-learning/clustering-when-you-should-use-it-and-avoid-it/#:~:text=Clustering%20is%20an%20unsupervised%20machine,more%20easily%20understood%20and%20manipulated.) \
builtIn: How to Form Clusters in Python: Data Clustering Methods (https://builtin.com/data-science/data-clustering-python) \
Google Developers: Clustering Algorithms (https://developers.google.com/machine-learning/clustering/clustering-algorithms) \
Analytics Vidhya: The Ultimate Guide to K-Means Clustering: Definition, Methods and Applications (https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-k-means-clustering/#What_Is_K-Means_Clustering?)


