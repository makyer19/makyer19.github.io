{
  "hash": "babc94555da8596f6b689ac07d5cf189",
  "result": {
    "markdown": "---\ntitle: Outlier Detection\nauthor: Michael Alex Kyer\ndate: 11/12/2023\nformat:\n  html:\n    code-fold: true\n---\n\n# What are Outliers\nOutliers, also known as anomalies, are defined as data entries that do not follow the typical trend of the rest of the data. While the outlier may be legitimate, it is inherently suspicious as oftentimes they are obtained due to a problem involved in the data collection.\n\n### Types of Outliers\nTypically, there are 3 different types of outliers.\n1. Point outliers are the types of outliers that are legitimate\n2. Contextual outliers are created due to issues of the data collection\n3. Collective outliers are points within a data set with the goal to identify other outliers\n\n### Importance of Removing or Ignoring Outliers\nOutliers can negatively affect the analysis of data sets they are found in. The main statistic affected is the mean value of the data set. As the mean is the average of the values of a random variable, it is subject to greater variety due to outliers. For example, let's think of a scenario where this might occur. We have 100 different cars, and we want to find the average number of miles per tank. That is multiplying the miles per gallon by the size of the fuel tank in gallons. Of these cars 99 of them fall within the expected range. However,  is one car, that is partly disassembled, without a fuel tank. Therefore, there is a data point where the miles per tank is 0. The mean miles per tank is likely in the multiple hundreds, but the one 0 entry is bound to drop the mean to a lower value than it should. Oftentimes, there is more nuance than the example here. For example, outliers in a medical dataset might reveal individuals with certain illnesses, and outliers in a tax data set could indicate fraud.\n\n### Methods of Outlier Detection\nThere are a multitude of outlier/anomaly detection algorithms available, so we will discuss some of the more common ones. \\\\\n1. Density based detection help identify points that differ from a normal, or dense, set of data. Points within dense regions are normal, and those in less dense or isolated regions are outliers. Example of density based detection algorithms include the isolation forest, and the K-nearest neighbors algorithm.\n2. Cluster based anomaly detection identify points that fall outside a cluster region. This might sound similar to the density based algorithms, but the key difference is that cluster based algorithms require a pre-defined number of clusters. This number can be optimized, but an incorrect number can result in the inclusion of outliers or the exclusion of key points. The K-means algorithm used in a previous blog post is one example of a clustering algorithm for anomaly detection. Our example will utilize a cluster algorithm which uses a standard scaler.\n3. Bayesian-network algorithms look to identify the probability that a given statistic will occur. Based on that probability, significant deviations are considered anomalies.\n4. Neural network algorithms involve building out a network to predict an expected series, then identify the deviations from that series.\n\n# Identifying Anomalies Utilizing a Cluster Based Algorithm\nFor this example, we will not be utilizing a dataset. Instead, we will synthesize our own data utilizing a sklearn package. Of course, an actual dataset could be used, but this will work to showcase the topic at hand.\n\n::: {.cell tags='[]' execution_count=1}\n``` {.python .cell-code}\nimport numpy as np \nimport pandas as pd\nfrom scipy import stats \nimport matplotlib.pyplot as plt \nimport matplotlib.font_manager \nfrom sklearn import metrics\nfrom sklearn.cluster import DBSCAN\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.preprocessing import StandardScaler\n```\n:::\n\n\n::: {.cell tags='[]' execution_count=2}\n``` {.python .cell-code}\ncenters = [[1, 1], [-1, -1], [1, -1]]\nX, labels_true = make_blobs(\n    n_samples=750, centers=centers, cluster_std=0.4, random_state=0\n)\n\nX = StandardScaler().fit_transform(X)\n\nplt.scatter(X[:, 0], X[:, 1])\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=582 height=411}\n:::\n:::\n\n\n### Observing Apparent Outlier\nIn this example, there are many points surrounding some apparent clusters. That being said, there are some noticable points that lie outside of the apparently dense clusters. We can use additional steps to view whether or not these points are in fact anomalies.\n\n::: {.cell tags='[]' execution_count=3}\n``` {.python .cell-code}\ndb = DBSCAN(eps=0.3, min_samples=10).fit(X)\nlabels = db.labels_\n\n# Number of clusters in labels, ignoring noise if present.\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\nn_noise_ = list(labels).count(-1)\n\nprint(\"Estimated number of clusters: %d\" % n_clusters_)\nprint(\"Estimated number of outliers: %d\" % n_noise_)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEstimated number of clusters: 3\nEstimated number of outliers: 18\n```\n:::\n:::\n\n\nAbove, we can see that there appear to be 3 main clusters, and roughly 18 outliers/anomalies. This is useful information in itself, but we can create a visual of these points as well.\n\n::: {.cell tags='[]' execution_count=4}\n``` {.python .cell-code}\nunique_labels = set(labels)\ncore_samples_mask = np.zeros_like(labels, dtype=bool)\ncore_samples_mask[db.core_sample_indices_] = True\n\ncolors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        # Black used for noise.\n        col = [0, 0, 0, 1]\n\n    class_member_mask = labels == k\n\n    xy = X[class_member_mask & core_samples_mask]\n    plt.plot(\n        xy[:, 0],\n        xy[:, 1],\n        \"o\",\n        markerfacecolor=tuple(col),\n        markeredgecolor=\"k\",\n        markersize=14,\n    )\n\n    xy = X[class_member_mask & ~core_samples_mask]\n    plt.plot(\n        xy[:, 0],\n        xy[:, 1],\n        \"o\",\n        markerfacecolor=tuple(col),\n        markeredgecolor=\"k\",\n        markersize=6,\n    )\n\nplt.title(f\"Estimated number of clusters: {n_clusters_}\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=582 height=431}\n:::\n:::\n\n\n### Analyze the Results\nLooking at the resulting scatter plot shown above, we can clearly see the 3 clusters labeled with red, yellow, and green. We are also able to view a multitude of black points. These are the points that correspond to the outliers. Essentially, these points are so far outside of the clusters, that they would be considered outliers. From here, a few different things can be done about these outliers. First, they can simply be removed to ensure cleaner results and plots. Additionally, these outliers can be analyzed to see why they might exist. Of course, this dataset is synthetic, but there could be valuable data associated with the outliers. The data collection mechanism could have problems, the analysis techniques could use some work, or maybe the outliers are the main focus of the data. Either way, anomaly/outlier detection is extremely important in machine learning and data analysis.\n\n### Conclusion\nAs we have shown, there are a ton of methods to identify outliers, and doing so can provide a lot of useful results. If you would like to learn more, here are some of the resources that I used in the creation of this blog post. \\\nGeeksforgeeks - Machine Learning for Anomaly Detection (https://www.geeksforgeeks.org/machine-learning-for-anomaly-detection/#) \\\nLinkedIn - What are the pros and cons of using clustering vs. density-based methods for anomaly detection? (https://www.linkedin.com/advice/3/what-pros-cons-using-clustering-vs-density-based-methods) \\\nSklearn - Demo of DBSCAN clustering algorithm (https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html)\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}