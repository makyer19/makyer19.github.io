[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/probability-theory/index.html",
    "href": "posts/probability-theory/index.html",
    "title": "Probability theory and random variables",
    "section": "",
    "text": "What is Probability Theory?\nProbability theory, as defined by Brown University, is “the mathematical framework that allows us to analyze chance events in a logically sound manner.” Chance events occur all the time, and they can be found in any instance where one or more possibilities can result. One common example is within flipping a coin. There is a 50% chance that the coin lands on heads, and 50% it lands on tails (assuming the coin is fair). Another classic example is drawing a card from a 52 card deck. There is a 1/52 chance of drawing any one specific card, but there are different odds for drawing a card of a specific color, suit, or kind. Now those are just common examples, so there are many others that we can observe.\n\n\nWhat are Random Variables?\nAlso defined by Brown University, a random variable is “a function that assigns a real number to each outcome in the probability space.” This definition is a little abstract in and of itself, but we can make it a bit clearer. Whenever any chance event occurs, there are multiple possible outcomes, and each outcome has a percent chance of occurring. Let’s apply this to our previous flip a coin example, more specifically, the odds of flipping heads. Here the random variable equation would be P(X = heads) = f(x). Here, P stands for ‘the probability of,’ and X is the random variable or the result of the chance event. Therefore, the left side of the equation reads as, ‘the probability that the chance event of a coin flip lands on heads.’ The right side of the equation is the function that gives us the resulting probability that the left hand side does occur. In the case of a coin flip, we know this function results in a flat 50% for heads or tails, but it can get much more complex than this.\n\n\nApplying Probability to a Naive Bayes Classifier\nThe Naive Bayes Classifier is a fairly simple model used in machine learning. The model essentially uses Bayes’ Theorem to generate its predictions. Bayes’ Theorem is an equation that is used to obtain the probability that the hypothesis h occurs given that D has also occurred. Think of h as the y variable, and D as the random variable. Let’s observe this in the lens of one of our earlier examples. The coin flip is not very interesting, so let’s instead use the draw a card. Let us think of the problem, “What is the probability that the card is red given that a queen was drawn?” Here, h is the ‘card is red,’ and D is the ‘given that a queen was drawn.’\n\nAbove we see the equation for Bayes’ Theorem. Of course, we can begin by looking at the card scenario, but instead we will look at the utilization of Bayes’ Theorem using penguins.\n\n\nNaive Bayes Classifier Applied to Penguins\nWe will attempt to use Bayes’ Theorem and a Naive Bayes Classifer to make predictions on a penguins species by the island they reside on, the culmen length, culmen depth, flipper length, and body mass. Before any of that, we will first import the dataset and all required libraries.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, f1_score\n\n\n\n\nCode\npenguins_data = pd.read_csv('penguins_size.csv')\npenguins_data = penguins_data.dropna()\n\n\n\n\nUse Scatterplots to Observe Best Predictors\nTo begin, we will utilize the Pandas scatter plot functions to create 4 scatter plots. Each scatter plot will plot a penguin’s characteristic against its species. We can then observe these plots to observe if we can find any useful predictors before even creating the model.\n\n\nCode\npenguins_data.plot.scatter(x='culmen_length_mm',y='species')\npenguins_data.plot.scatter(x='culmen_depth_mm',y='species')\npenguins_data.plot.scatter(x='flipper_length_mm',y='species')\npenguins_data.plot.scatter(x='body_mass_g',y='species')\n\n\n&lt;Axes: xlabel='body_mass_g', ylabel='species'&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing the Scatter Plots\nLooking at the scatter plots, we find multiple useful predictors. First of all, it seems quite easy to differentiate between Gentoo and Adelie penguins as each of their measured traits appear to oppose one another. Adelie have a lower body mass while Gentoo are heavier. Adelie have shorter flippers and culmen, while Gentoo have longer flippers and culmen. Finally, Adelie generally have greater culmen depth while Gentoo do not. Therefore, a penguin with a low body mass, short flipper, short culmen, and large culmen depth, is likely to be an Adelie. However, Chinstrap penguins tend to be quite similar to Adelie penguins in each measurment, except for culmen length which is more similar to the Gentoo. Therefore, by these scatterplots alone, we can determine that, assuming the model functions correctly, the accuracy should be relatively high as the predictors seem to be quite good. Remember that Bayes’ Theorem is essentially predicting y based on a hypothesis given that some data has already occurred. In our case, the model will be asked questions such as “What is the probability that the penguin is of the Adelie species given a low body mass, shorted flipper length, short culmen length, and a large culmen depth?” Using multiple instances of Bayes’ Theorem, we can say there is a high probability that the given penguin is in fact of the Adelie species. This is just one example, but it is essentially how the model will work behind the scenes.\n\n\nUsing a Naive Bayes Classifier to Predict a Penguins Species\nNow that we have looked at the raw data, we will use that data to create a model to predict a penguin’s species by their measurments. To begin, we will replace all of the non numerical data with numerical data to allow it to be used in the model creation. Then we will split the data into training and test sets using train_test_split(). Then we will create a GaussianNB (Gaussian Naive Bayes) model, and train it. Finally, we will make our predictions using the model and analyze the results.\n\n\nCode\npenguins_data['species'] = penguins_data['species'].replace({\"Adelie\":0,\"Chinstrap\":1,\"Gentoo\":2})\npenguins_data['island'] = penguins_data['island'].replace({\"Torgersen\":0,\"Biscoe\":1,\"Dream\":2})\npenguins_data['sex'] = penguins_data['sex'].replace({\"MALE\":0,\"FEMALE\":1,\".\":1})\npenguins_data.head()\n\nX = penguins_data.drop('species', axis=1)\ny = penguins_data['species']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=125)\n\nmodel = GaussianNB()\n\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\naccuray = accuracy_score(y_pred, y_test)\nprint(\"Accuracy:\", accuray)\n\ncm = confusion_matrix(y_test, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot();\n\n\nAccuracy: 0.9369369369369369\n\n\n\n\n\n\n\nAnalyzing the Results of the Model\nLooking at the results of the model, we see that it performed relatively well. We can say this by comparing it to a Naive Dummy Classifier which would make a random prediction each time. By that, a Dummy Classifier would be correct ~33% of the time as each prediction would be random. Comparing the 33% to our model’s ~93% accuracy, we can say that our model does perform relatively well. We can also create a confusion matrix to see where our model went wrong. The diagonal from the top left to bottom right are the correctly predicted values, whereas every other box is an incorrect value. Most of these boxes have values of 0, meaning no wrong predictions were made. However, the center box on the top row does have a significant number of incorrectly predicted entries. This box is reserved for entries that were predicted to be Chinstrap penguins, but they were actually Adelie penguins. This makes sense, as the only predictor that differentiates a Chinstrap from an Adelie is the penguin’s culmen length. Chinstrap penguins typically have a longer culmen than Adelie, but if a Chinstrap happens to have an culmen on the shorter side, then the prediction between Chinstrap and Adelie becomes all but random.\n\n\nConclusion\nAfter discussing the basics of probability theory, random variables, and Bayes’ Theorem, we can see how all of that can be utilized in the context of machine learning. Of course, this is only the tip of the statistical/machine learning iceberg, but if you would like to observe some of the resources I used in the creation of this blog post, I will link them here. Pengiun Dataset - https://www.kaggle.com/datasets/parulpandey/palmer-archipelago-antarctica-penguin-data/ Brown University Probability Theory - https://seeing-theory.brown.edu/basic-probability/index.html Naive Bayes Classifier Learning Tutorial - https://www.datacamp.com/tutorial/naive-bayes-scikit-learn"
  },
  {
    "objectID": "posts/clustering/index.html",
    "href": "posts/clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "What is Clustering\nClustering is a method of unsupervised machine learning. The goal is to group, or cluster, sets of data points within a distribution. Based on these different clusters, information can be inferred as to what it means to be in one group or another.\n\nClustering Compared to Classification\nAt a first glance, clustering and classification may appear to be quite similar. The difference lies in that while classification tends to have predefined sets of categories that points will be placed into, clustering instead groups points by similarities. Therefore, as classification uses predefined labels, it is typically used in supervised learning. On the other hand, clustering data is not labelled, and so it is used during unsupervised learning.\n\n\nTypes of Clustering\nThere are multiple different methods of clustering.\nCentroid based clustering is a common clustering technique that allocates a center to each cluster. Each point is measured from a distance to each center, and it is placed in the cluster whose center it is closest to. We will be using the K-means algorithm for our clustering example later, and we will explain what that is in the next section.\nDensity based clustering places points of high density into randomly constructed distributions. To further utilize density, no outliers are observed or placed within the clusters.\nDistribution based clustering takes input data of distributions and takes the distance from each point to the center of each other distribution. The probability that a point is within a certain distribution is lower the further it is from a distributions center.\nHierarchical clustering involves creating a tree of clusters based on heirarchical data. This often leads to clusters being placed within other clusters.\n\n\n\nThe K-means Algorithm\nAs stated previously, the K-means algorithm is a centroid based clustering algorithm that separates the points into K different clusters each with a center. In this case, the centroids are determined at random, and each centroid is chosen from the list of points given. Therefore, the biggest variable here is the number, K, clusters we are looking for. The algorithm can be optimized easily by repeatedly changing K until the sum of the squared distance between each point and the closest centroid is minimized. After randomly selecting the centroids, each point is placed into the cluster of its nearest centroid. From there, the actual center of each cluster is calculated by averaging coordinate values, and those centers become the new centroids. Finally, each point is again placed into the cluster that the nearest centroid is assigned to.\n\n\nClustering Example: Grouping the Penguins\nSimilarly to some previous examples, we will once again use the penguins data set. At face value, each penguin seems to have a predefined label in either its island or species. However, we can easily ignore these values in order to utilize an unsupervised, clustering approach. Like always, we will begin with some pre-processing.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\n\npenguins_data = pd.read_csv('penguins_size.csv')\npenguins_data = penguins_data.dropna()\n\npenguins_data = penguins_data.drop('species', axis=1)\npenguins_data = penguins_data.drop('island', axis=1)\npenguins_data = penguins_data.drop('sex', axis=1)\n\n\n\nUsing K-means to Cluster the Data\nNext, we will cluster the data based off two variables of our numerical data. Those input data being culmen length, culmen depth, body mass, and flipper length. Recalling from our previous probability distribution blog post, in order to differentiate between Adelie and Chinstrap penguins, we should have one of the categories be culmen length. This might seem like it falls into the classification category, but this new set of data does not have any classifier information. We’ll set our number of clusters to 3. This can be done with some optimization, but we already know from previous experience that we have 3 species of penguin in or dataset, so we can skip that step here. Now we can use our K-means algorithm which works well for two reasons. First, K-means is a very fast algorithm that executes in O(n) time where n is the number of points given. Secondly, this can be done for a very large dataset in which there is not enough time to place each penguin into a category of species or island. We can then create multiple plots which cluster our data based on two variables in each way possible, but we will just display the cluster on culmen length and flipper length. This will help to avoid confusion in our analysis.\n\n\nCode\nX = penguins_data[['flipper_length_mm','culmen_length_mm']]\n\nkmeans = KMeans(n_clusters=3, n_init=10)\nkmeans.fit(X)\ny_kmeans = kmeans.predict(X)\n\nplt.scatter(X.loc[:, 'flipper_length_mm'], X.loc[:, 'culmen_length_mm'], c=y_kmeans, s=50, cmap='viridis')\ncenters = kmeans.cluster_centers_\nplt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)\nplt.xlabel('Flipper Length (mm)')\nplt.ylabel('Culmen Length (mm)')\nplt.title('Clustering on Culmen Length and Flipper Length')\nplt.show()\n\n\n\n\n\n\n\nAnalyzing the Results\nObserving the cluster scatter plot shown above, we can clearly see the 3 different clusters. As the colors may change, I will refer to them as bottom left, middle, and upper right. As we do not have any classifiers, we are not positive as to what the clusters truly mean. They may refer to species or island, but we cannot be immediately sure. What we do know, is that the bottom left penguins have short culmen and flippers, the upper left have long culmen and flippers, and the middle have medium length flippers with longer culmen. From that we can infer that each grouping of penguin may have something in common. Indeed, the bottom left is majorly comprised of Adelie penguins, the middle of Chinstrap, and the upper left of Gentoo. Without that information though, we can begin making predictions, or using that data to our advantage. Maybe certain penguins are vulnerable to an illness, and a good identifier are penguins with short flippers and culmen. We don’t know the type of those penguins, but we can still identify the penguins within that cluster as potentially at risk. This is just one example of how clustering can be used to gather useful information for data without classifiers.\n\n\n\nConclusion\nWith all this in mind, we can see how clustering can be utilized in machine learning to make predictions or analyze data that do not have any specified clusters. Whether the data set is too large to label each point with a classifier, or there are no classifiers available, we can still analyze the data in a useful way. I hope you were able to learn some information about clustering from this blogpost. If you would like to learn more, you can visit these resources that I used in the formulation of this blogpost.\nPengiun Dataset - (https://www.kaggle.com/datasets/parulpandey/palmer-archipelago-antarctica-penguin-data/)\nexplorium: Clustering — When You Should Use it and Avoid It (https://www.explorium.ai/blog/machine-learning/clustering-when-you-should-use-it-and-avoid-it/#:~:text=Clustering%20is%20an%20unsupervised%20machine,more%20easily%20understood%20and%20manipulated.) builtIn: How to Form Clusters in Python: Data Clustering Methods (https://builtin.com/data-science/data-clustering-python) Google Developers: Clustering Algorithms (https://developers.google.com/machine-learning/clustering/clustering-algorithms) Analytics Vidhya: The Ultimate Guide to K-Means Clustering: Definition, Methods and Applications (https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-k-means-clustering/#What_Is_K-Means_Clustering?)"
  },
  {
    "objectID": "posts/error-analysis/index.html",
    "href": "posts/error-analysis/index.html",
    "title": "Error Analysis",
    "section": "",
    "text": "How to Perform an Error Analysis\nWhen performing an error analysis, there is a multitude of things to look for in determining the strength of your model. When a model is created, it will train by mapping a set of x-values to a set of y-values. These sets are known as the training sets. Two other sets are set aside to be used as test sets in order to test the model. The test set of x-values can then be passed into the model to get a set of predicted y-values. Finally, these predicted y-values are compared to the other set of set aside y-values to determine the strength of the model. One of the best ways to compare the predicted y-values to actual y-values is throught the utilization of a confusion matrix.\n\n\nWhat is a Confusion Matrix\nIn essence, a confusion matrix is a plot that maps a set of actual values to the set of predicted values. Here, the x axis is typically attributed to the predicted label, amd the y axis is typically attributed to the true label. To begin we will first discuss how a confusion matrix can assist the error analysis of a model trained with a binary classifier. In this case, a predicted, true y-value pair could have 4 options. The value was predicted to be false, and it was actually false; the value was predicted to be false, and it was true; the value was predicted to be true, and it was false; and the value was predicted to be true, and it was true. Before explaining further, we will create and example to show.\n\n\nCreating a Confusion Matrix - Determining a Penguin’s Sex By Size\n\nImport the Libraries\n\n\nCode\nimport sys\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split\n\n\n\n\nPreprocessing\n\n\nCode\npenguins_data = pd.read_csv(\"penguins.csv\")\n\npenguins_data = penguins_data.drop('studyName', axis=1)\npenguins_data = penguins_data.drop('Sample Number', axis=1)\npenguins_data = penguins_data.drop('Species', axis=1)\npenguins_data = penguins_data.drop('Region', axis=1)\npenguins_data = penguins_data.drop('Island', axis=1)\npenguins_data = penguins_data.drop('Stage', axis=1)\npenguins_data = penguins_data.drop('Individual ID', axis=1)\npenguins_data = penguins_data.drop('Date Egg', axis=1)\npenguins_data = penguins_data.drop('Comments', axis=1)\n\npenguins_data.dropna(inplace=True)\n\npenguins_data['Sex'] = penguins_data['Sex'].replace({\"MALE\":0,\"FEMALE\":1,\".\":1})\npenguins_data['Clutch Completion'] = penguins_data['Clutch Completion'].replace({\"No\":0,\"Yes\":1})\n\ndef makeBinary(value, median):\n    if int(value) &lt;= median:\n        return 0\n    else:\n        return 1\n    \npenguins_data['Body Mass (g)'] = penguins_data['Body Mass (g)'].apply(makeBinary, median=penguins_data['Body Mass (g)'].median())\n\nX = penguins_data.drop('Sex', axis=1)\ny = penguins_data['Sex']\n\n\n\n\nTraining and Testing our Model\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nrandomForest = RandomForestClassifier()\nrandomForest.fit(X_train, y_train)\n\ny_prediction = randomForest.predict(X_test)\n\nconfusionMatrix = confusion_matrix(y_test, y_prediction)\n\nConfusionMatrixDisplay(confusion_matrix=confusionMatrix).plot();\n\n\n\n\n\n\n\nAnalyzing the Confusion Matrix\nAbove we can see the confusion matrix of our model that determines the sex of a penguin based on a variety of factors such as flipper length, body mass, etc. We can simply observe this using the plot, but it becomes easier to analyze once we define a couple terms. Before that, let us first talk about the matrix in general. By the labels, the x axis is the predicted label while the y is the true label. The top left box is reserved for entries that were predicted to be negative, and the actual result was negative. The top right is for entires that were predicted positive, but they were actually negative. The bottom left is for entries predicted negative, but they were actually positive. Finally, the bottom right is for entries predicted positive, and they were actually positive. For our model specifically, negative corresponds to a male pengiun, and positive corresponds to a female penguin.\n\n\nAccuracy\nAccuracy is the test of overall correctness. To calculate the accuracy value, it is the number of correct prediction over the total number of predictions. This is the most general way to determine how good your model is at correctly predicting values.\n\\({\\displaystyle \\mathrm {ACC} ={\\frac {\\mathrm {TP} +\\mathrm {TN} }{\\mathrm {P} +\\mathrm {N} }}={\\frac {\\mathrm {TP} +\\mathrm {TN} }{\\mathrm {TP} +\\mathrm {TN} +\\mathrm {FP} +\\mathrm {FN} }}}\\)\nExplained in words, the value in the top left and the bottom right is divided by the sum of all values.\n\n\nCode\naccuracy = accuracy_score(y_test, y_prediction)\nprint(\"Accuracy:\", accuracy)\n\n\nAccuracy: 0.8615384615384616\n\n\nAs we can see, our model has a ~90% accuracy. We can determine how good this is by comparing it to a naive model. A naive model would likely pick positive every time, negative every time, or a random value every time. As this is a binary classifier, the naive model would likely have something close to a 50% success rate. Comparing this to our model with ~90% accuracy, we can say that our model is performing fairly well.\n\n\nPrecision\nPrecision is the comparison of true positives to the number of all predicted positives.\n\\({\\displaystyle \\mathrm {P} ={\\frac {\\mathrm {TP} }{\\mathrm {TP} +\\mathrm {FP} }} }\\)\n\n\nCode\nprecision = precision_score(y_test, y_prediction)\nprint(\"Precision:\", precision)\n\n\nPrecision: 0.8181818181818182\n\n\nOur precision is slightly higher than our accuracy at ~92% precision. With our model, this means that if we predict a penguin is a female, it will be right 92% of the time. To show this is important, let’s say there is to be a study on penguin births, and so all female penguins are to be tagged. These tags are quite expensive, so it is important that we don’t waste any on male penguins. With a 92% precision, we can be confident that we will not waste that many tags!\n\n\nRecall/Sensitivity\nRecall is similar to precision, but there is one key difference. Instead of comparing true postives to all predicted positives, we are looking at comparing all true positives to all actual positives. It should also be noted that recall has other terms that can be used interchangably such as sensitivity and true positive rate.\n\\({\\displaystyle \\mathrm {TPR} ={\\frac {\\mathrm {TP} }{\\mathrm {P} }}={\\frac {\\mathrm {TP} }{\\mathrm {TP} +\\mathrm {FN} }}=1-\\mathrm {FNR} }\\)\n\n\nCode\nrecall = recall_score(y_test, y_prediction)\nprint(\"Recall:\", recall)\n\n\nRecall: 0.9\n\n\nOur recall value is ~86%. Let’s put this number in the context of our model and the same scenario used to describe precision. We want to tag all female penguins as to not lose any data. With this recall, we wll tag 86% of all females.\n\n\nSpecificity\nThe inverse of recall or sensitivity is specificity. This is also known as the true negative rate.\n\\({\\displaystyle \\mathrm {TNR} ={\\frac {\\mathrm {TN} }{\\mathrm {N} }}={\\frac {\\mathrm {TN} }{\\mathrm {TN} +\\mathrm {FP} }}=1-\\mathrm {FPR} }\\)\nThere is no sklearn metric function for calculating specificity, but we can easily calculate it using the function above. This would be useful in the context of our model for in a scenario like the one discussed for recall, but instead of females it would be males.\n\n\nHow To Use These Values\nOftentimes we may want to use a model that prioritizes certain values. Accuracy is likely always desired to be high, but what about precision or recall. Let’s think about this in the context of our previous tagging example. Before we stated that the tags we quite expensive, so let’s keep that as the case. With that in mind, precision would likely be prioritized more than recall. The reason for this is that we care more about not waste expensive tags on males than having every female tagged. On the other hand, let’s say that tags are very inexpensive and a lot of them are available. In this case, we would prefer to have as much data as possible by tagging every female, and allowing some males to be tagged as well. This scenario would prefer recall over precision. In general though, we would prefer each of these values to be high.\n\n\n\nNon Binary Classifier Models\nOf course, some models have multiple possible y-values. In these cases, it is just as easy to make the confusion matrix, but it is slightly harder to interpret. With the sklearn metrics library, the functions given make it simple, but in general it is harder to the naked eye. It is still possible by taking the sum of multiple boxes instead of just one, but it is much easier to use the functions provided.\n\n\nConclusion\nWith all of these terms in mind, we can much more easily analyze our models and how good they are in different areas. Creating a confusion matrix is easy, but analyzing it is a bit difficult to the naked eye. That being said, with the use of some formulas we can calculate the accuracy, precision, recall, and a multitude of other values not discussed here. These values make analysis of the model much easier. Of course, we would prefer our model to have great accuracy, precision, and recall, but that is likely not always possible. In general it is best to prioritize accuracy, but there are situation when precision or recall would be preferred over the other.\nIf you would like to learn more about error analysis, you can look into these other sources I used in the writing of this blogpost.\nMage - Guide to accuracy, precision, and recall (https://www.mage.ai/blog/definitive-guide-to-accuracy-precision-recall-for-product-developers) Wikipedia - Confusion Matrix (https://en.wikipedia.org/wiki/Confusion_matrix) Analytics Vidhya - Precision and Recall | Essential Metrics for Machine Learning (2023 Update) (https://www.analyticsvidhya.com/blog/2020/09/precision-recall-machine-learning/)"
  },
  {
    "objectID": "posts/ensemble-models/index.html",
    "href": "posts/ensemble-models/index.html",
    "title": "Ensemble Models",
    "section": "",
    "text": "What is an Ensemble Model\nSimply put, ensemble models are the culmination of multiple other models with the goal of strengthening the overall prediction capabilities. A lot of issues can arise when only utilizing one model. We’ll look into what those issues are later, but for now let’s use an analogy of a visit to the doctor to describe the basics of ensemble models. Let’s say that you are suffering from an undiagnosed ailment, and you would like to figure out what it is. To do so, you’ll take a trip to your doctor’s office, and receive a diagnosis. There they tell you that it is probably nothing. However, you are not convinced of this diagnosis, so you travel to another doctor for a second opinion. This one diagnoses you with a life threatening condition, and you need immediate treatment. Now you have two very different diagnoses, so you might seek out a third, fourth, maybe even fifth opinion to really get to the bottom of what this issue is. In the end, you’ll follow the procedure and diagnosis that most doctors you saw agreed upon. Machine learning is very much similar with algorithms predicting an outcome like the doctors were providing a diagnosis. With a single model, you can receive inaccurate results as a doctor provided an incorrect diagnosis. Ensemble models solve this issue just like gathering the opinion of other doctors can provide a more accurate diagnosis. Firstly, an ensemble model combine a multitude of individual models. Then a single input is passed through each individual model, so a collection of outputs are produced. These outputs are the predictions, and the ensemble model can select its overall prediction from the collection. Most likely, the selected prediction will be either the most frequently appearing prediction from the individual models, or it is some average value of the predictions. Now, we can see that the utilization of ensemble models solves the issues of single models by potentially increasing accurracy while lowering the error.\n\nWhat Are Decision Trees\nFor each of the models discussed here, they will create a multitude of smaller decision tree models. One of the more common types of decision tree is a binary decision tree. This is a tree structure where each inner node is a yes/no decision to make, and each node has two branches, one yes and one no. An input value is passed in and the decisions are made one by one down the tree until a leaf node is eventually reached. These leaf nodes contain the final prediction that the model will make for the given input.\n\n\n\nCommonly Used Ensemble Techniques\nBoth of the techniques discussed can be applied to regression or classification trees with subtle differences in each. Using regression trees will oftentimes have the selected prediction being the average of the individual models. On the other hand, the ensemble will take the most frequent occurrence if classification trees are used. ### Bagging The first of the two techniques we will discuss is known as bagging or bootstrap aggregation. Here there is one training set, and a multitude of tree models will be constructed from that training set. The difference between the models is that each one is trained with a different subset of the training set. Each subset will be equal in size, and the values in each will reflect the values of the training set. However, to make each subset unique and equal in size, it will allow for repeated entries. The result is a set of unique subsets which train a set of tree models of the same size. These models then each individually predict the outcome of an input, and either the average or most common prediction is chosen. ### Random Forest A random forest ensemble model is very similar to the bagging ensemble model, but there is one key difference. Each node can only be split on a random selection of predictors. With most datasets, certain predictors are more significant than others, and this approach means that the more significant predictors might be found lower on the tree. Now a collection of these models will produce very different results. With these different models, the same approach of taking the average or most common prediction of these models can then be used.\n\n\nRandom Forest Ensemble Model: An Analysis of Billionaires Amongst Billionaires\nIn this example, we are going to analyze a dataset of billionaires and their various statistics including age, whether they are self-made, and a variety of statistics regarding their country of citizenship. Our goal is to see if we can use this data to create a model that accurately predicts whether or not a given billionaire is in the upper half of other billionaires in terms of their net worth. ### Importing Libraries\n\n\nCode\nimport sys\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split\nfrom scipy.stats import randint\n\n\n\nPreprocessing Data\nFor this next step, we will have to do some preprocessing of the data in order to turn each of the values of the table into either a binary 0 or 1. 0s will correspond to a no answer, and 1s will correspond to a yes answer. First we will drop each of the tables that are purely textual including their name, country, city, title, gender, and so on. There is likely very useful information within the dropped data, but for now we will solely focus on the numerical inputs. Then we will utilize a binary classifier to change each entry to either a 0 or a 1. The classifier will be if the entry is less than or equal to the median for 0, or greater than the median for 1. After this step we will be able to begin our model creation, training, and prediction steps.\n\n\nCode\nbillionaire_data = pd.read_csv(\"billionaire.csv\")\n\nbillionaire_data = billionaire_data.drop('finalWorth', axis=1)\nbillionaire_data = billionaire_data.drop('category', axis=1)\nbillionaire_data = billionaire_data.drop('personName', axis=1)\nbillionaire_data = billionaire_data.drop('country', axis=1)\nbillionaire_data = billionaire_data.drop('city', axis=1)\nbillionaire_data = billionaire_data.drop('industries', axis=1)\nbillionaire_data = billionaire_data.drop('source', axis=1)\nbillionaire_data = billionaire_data.drop('countryOfCitizenship', axis=1)\nbillionaire_data = billionaire_data.drop('organization', axis=1)\nbillionaire_data = billionaire_data.drop('lastName', axis=1)\nbillionaire_data = billionaire_data.drop('firstName', axis=1)\nbillionaire_data = billionaire_data.drop('title', axis=1)\nbillionaire_data = billionaire_data.drop('date', axis=1)\nbillionaire_data = billionaire_data.drop('state', axis=1)\nbillionaire_data = billionaire_data.drop('residenceStateRegion', axis=1)\nbillionaire_data = billionaire_data.drop('birthDate', axis=1)\nbillionaire_data = billionaire_data.drop('status', axis=1)\nbillionaire_data = billionaire_data.drop('gender', axis=1)\nbillionaire_data = billionaire_data.drop('gdp_country', axis=1)\n\nbillionaire_data.dropna(inplace=True)\n        \ndef makeBinary(value, median):\n    if int(value) &lt;= median:\n        return 0\n    else:\n        return 1\n\ntoBinaryColumns = ['rank', 'cpi_country', 'cpi_change_country', 'gross_tertiary_education_enrollment', 'gross_primary_education_enrollment_country',\n                  'life_expectancy_country', 'tax_revenue_country_country', 'total_tax_rate_country', 'population_country', 'latitude_country', \n                  'longitude_country']\n\nfor columnName in toBinaryColumns:\n    billionaire_data[columnName] = billionaire_data[columnName].apply(makeBinary, median=billionaire_data[columnName].median())\n    \nbillionaire_data['selfMade'] = billionaire_data['selfMade'].map({False:0,True:1})\n\nX = billionaire_data.drop('rank', axis=1)\ny = billionaire_data['rank']\n\n\n\n\nInitial Fit and Prediction\nHere we will split our data into a train and test set. We will first train our model so that it can perform a series of predictions. We will then use the trained model to predict the answers to our test set, and compare them to their true result.\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nrandomForest = RandomForestClassifier()\nrandomForest.fit(X_train, y_train)\n\ny_prediction = randomForest.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_prediction)\nprint(\"Accuracy:\", accuracy)\n\n\nAccuracy: 0.5791666666666667\n\n\n\n\nAttempt to Improve our Accuracy\nNext we will attempt to improve our accuracy by passing in random sets of hyperparameters, and calculates the score for each. The set with the highest score is then saved to be used to make predictions later.\n\n\nCode\nparam_dist = {'n_estimators': randint(50,500),\n              'max_depth': randint(1,20)}\n\nrandomForest = RandomForestClassifier()\n\nrandomSearch = RandomizedSearchCV(randomForest, param_distributions = param_dist, n_iter=5, cv=5)\n\nrandomSearch.fit(X_train, y_train)\n\nbest_estimators = randomSearch.best_estimator_\n\nprint('Best hyperparameters:',  randomSearch.best_params_)\n\n\nBest hyperparameters: {'max_depth': 5, 'n_estimators': 335}\n\n\n\n\nUsing the Best Hyperparameters to Make Predictions\nWith the set of the best hyperparameters, we will again test the model against our set aside testing data, and compare the predicted results to their actual values. We will first show our results in the form of a confusion matrix. Afterwards, we will use a bar graph to show the most important predictors in determining the correct value.\n\n\nCode\n# Generate predictions with the best model\ny_prediction = best_estimators.predict(X_test)\n\n# Create the confusion matrix\nconfusionMatrix = confusion_matrix(y_test, y_prediction)\n\nConfusionMatrixDisplay(confusion_matrix=confusionMatrix).plot();\n\naccuracy = accuracy_score(y_test, y_prediction)\nprecision = precision_score(y_test, y_prediction)\nrecall = recall_score(y_test, y_prediction)\n\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\n\n\nAccuracy: 0.5854166666666667\nPrecision: 0.5843137254901961\nRecall: 0.6157024793388429\n\n\n\n\n\n\n\nCode\npredictorPlot = pd.Series(best_estimators.feature_importances_, index=X_train.columns).sort_values(ascending=False)\npredictorPlot.plot(x=\"year\", y=\"people\")\nplt.xlabel(\"year\",  size = 20)\nplt.ylabel(\"Mean decrease in impurity\", size = 20)\nplt.title(\"Feature Importances of Determining Upper Half of Billionaries\", size = 25)\n\npredictorPlot.plot.bar();\n\n\n\n\n\n\n\nInterpreting the Data\nObserving the data above, and running the creation and prediction of the model a few times, we can see that there is improvement by tuning the models, if only slightly. When tuning our model to the optimal hyperparameters, we see a roughly 2% increase in accuracy for predicting whether or not a billionaire was in the upper echelon of other billionaires. As it is a binary prediction, a naive dummy classifier could predict either a yes or no everytime and receive a 50% success rate. Our results are better, but only slightly.\nWe can observe the confusion matrix above to determine a multitude of values, but we will discuss more in depth what these values mean in a different blog post. For now we will just define the values listed above and how they can be interpreted.\nAccuracy: The number of predictions that were correct Positive Precision: The number of true positives compared to the number of all predicted positives Positive Recall: The number of true positives compared to the number of all actual positives\nLooking at the most important predictors, the ones frequently labelled as the most important, are those centered around age. Birthyear, age, birthday, are always among the top predictors. This makes sense as those who are older have more time to accrue wealth than those who are younger.\n\n\n\nWrap Up\nThat concludes this blogpost on ensemble models. If you would like to continue learning about ensemble models in general, other models, or either of the models discussed here, I have provided a list of articles used in the creation of this blogpost\nbuiltIn - Ensemble Models: What Are They and When Should You Use Them? (https://builtin.com/machine-learning/ensemble-model) dataiku’s Blog - Tree-Based Models: How They Work (In Plain English!) (https://blog.dataiku.com/tree-based-models-how-they-work-in-plain-english#:~:text=Ensemble%20models%20can%20be%20used,non%2Dlinear%20relationships%20quite%20well.) datacamp - Random Forest Classification with Scikit-Learn (https://www.datacamp.com/tutorial/random-forests-classifier-pythonRandom Forest Classification with Scikit-Learn)\nBillionaires Dataset Used - Billionaires Statistics Dataset (2023) by Nidula Elgiriyewithana (https://www.kaggle.com/datasets/nelgiriyewithana/billionaires-statistics-dataset/data)"
  },
  {
    "objectID": "posts/linear-regression/index.html",
    "href": "posts/linear-regression/index.html",
    "title": "Linear Regression Blogpost",
    "section": "",
    "text": "What is Regression\n\nSimply put, regression is a method used in data analysis which compares one or more independent variables to a single dependent variable. To further define this term, let’s use the analogy of a final exam for a difficult test. The dependent variable here will be the score of the exam, and there can be a multitude of factors, or independent variables, that play into what score you get. The time spent studying for the exam, the number of lectures attended, the amount of sleep the night before, what was ate the day of the exam, other stressful events currently going on, and so on. Now assume you have data on all of these factors for a multitude of students. Some questions you might have include Which factors will affect the final score the most? Will some factors not affect the score at all? We can use regression to answer these questions. A regression model will create a function that predicts the result of the dependent variable from the value of the independent variable.\n\nHow to Use a Regression Model\n\nTo create the aforementioned function, a plot is created with the y values mapping to the dependent variable, and the x values mapping to the independent variable. In our final exam analogy, we would have a multitude of plots. Assuming a sample of 100 students, one plot would have 100 points mapping the hours spent studying (x value) to their score on the final exam (y value). Another could map the number of lectures attended to the final exam score. A third could map the calories eaten before the exam to the final exam score. These plots could go on and on. For each plot created, we can use regression to generate the aforementioned function, and this function can be used to determine a correlation between each independent variable, and the dependent variable. It is also important to note that there are multiple different kinds of regression including linear, multiple, non-linear, and many more. We will use a linear regression model as it is the most simple, and the most frequently used. That being said, a multiple regression model could be used for our previous example to simultaneously analyze a multitude of independent variables on their effect on the final exam score.\n\nLinear Regression Example: An Analysis of Red Wine Quality\n\nIn this next example, we will use code to perform a regression analysis on data provided by UC Irvine on how a variety of factors affect red wine quality.\n\n\nCode\nimport sys\n!{sys.executable} -m pip install ucimlrepo\nimport ucimlrepo\nfrom ucimlrepo import fetch_ucirepo \nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.linear_model import LinearRegression\n\n\nRequirement already satisfied: ucimlrepo in /Users/alexkyer/anaconda3/lib/python3.11/site-packages (0.0.1)\n\n\n\n\nCode\nwine_quality = pd.read_csv(\"winequality-red.csv\")\n\nalcohol = []\ntarget = []\nfor index, row in wine_quality.iterrows():\n    values = row.values[0].split(';')\n    target.append(float(values[-1]))\n    alcohol.append(float(values[-2]))\n\nd = {'Alcohol Percentage': alcohol, 'Quality Score': target}\nwine_dataframe = pd.DataFrame(data=d)\n\nX = wine_dataframe[[\"Alcohol Percentage\"]].values\ny = wine_dataframe[[\"Quality Score\"]].values\n\n# Visualize the data\nwine_dataframe.plot(kind='scatter', grid=True,\n             x=\"Alcohol Percentage\", y=\"Quality Score\")\nplt.axis([8, 15, 2, 10])\nplt.show()\n\n\n\n\n\nWith the above plot, we observe a graph representation of the wine dataset. We have plotted a point for each entry in the provided CSV file with an x-value equal to the alchohol percentage, and a y-value equal to the quality score. Based on observing the visual graph alone, we can begin to make assumptions regarding the data, but we can go further by utilizing regression.\n\n\nCode\nmodel = LinearRegression()\n\nmodel.fit(X, y)\n\nprint(\"The correlation of alcohol percent to wine quality is %.2f\" % math.sqrt(model.score(X, y)))\n\n# Make a prediction for Cyprus\nplt.scatter(X, y,color='b')\nplt.xlabel(\"Alcohol Percentage\")\nplt.ylabel(\"Quality Score\")\nplt.plot(X, model.predict(X),color='r') # outputs [[6.30165767]]\nplt.show()\nX_1 = [[8.5]]\nX_2 = [[11.1]]\nX_3 = [[14.7]]\nprint(\"The predicted score for a red wine with an alcohol percentage of 8.5 is %.2f\" % model.predict(X_1)[0][0])\nprint(\"The predicted score for a red wine with an alcohol percentage of 11.1 is %.2f\" % model.predict(X_2)[0][0])\nprint(\"The predicted score for a red wine with an alcohol percentage of 14.7 is %.2f\" % model.predict(X_3)[0][0])\n\n\nThe correlation of alcohol percent to wine quality is 0.48\nThe predicted score for a red wine with an alcohol percentage of 8.5 is 4.94\nThe predicted score for a red wine with an alcohol percentage of 11.1 is 5.88\nThe predicted score for a red wine with an alcohol percentage of 14.7 is 7.18\n\n\n\n\n\nUtilizing the sklearn library, we can gather additional information regarding the relationship between alcohol percentage and the quality score given to the wine. First, we observe the correlation of these variables is approximately 0.48. This means that there is a moderate, positive correlation between these values. This fact is further shown when plotting a regression line on top of the graph. We see that this line is at a positive diagonal, and here we can infer that a red wine with a higher alcohol percentage will likely have a higher score than a red wine with a lower alcohol percentage. This fact is further illustrated by utilizing the sklearn predict method. This method will use the existing data and the regression line to predict the y-value of an x-value added to the set. Here, we have predicted the quality score for red wines with an alcohol percentage of 8.5, 11.1, and 14.7. The predicted scores increased as the theoretical alcohol percetage values increased, and this is yet another way to visualize the relationship between alcohol percentage and the quality score of red wine.\nWith this in mind, let’s repeat this process with another independent variable provided in the data set, the amount of residual sugar. Repeating this process can provide us with a correlation between the amount of residual sugar and the quality score.\n\n\nCode\nresidual_sugar = []\nfor index, row in wine_quality.iterrows():\n    values = row.values[0].split(';')\n    residual_sugar.append(float(values[3]))\n\nd = {'Amount of Residual Sugar': residual_sugar, 'Quality Score': target}\nwine_dataframe = pd.DataFrame(data=d)\n\nX = wine_dataframe[[\"Amount of Residual Sugar\"]].values\ny = wine_dataframe[[\"Quality Score\"]].values\n\n# Visualize the data\nwine_dataframe.plot(kind='scatter', grid=True,\n             x=\"Amount of Residual Sugar\", y=\"Quality Score\")\nplt.axis([0, 17.5, 2, 10])\nplt.show()\n\nmodel = LinearRegression()\n\nmodel.fit(X, y)\n\nprint(\"The correlation of amount of residual sugar to wine quality is %.2f\" % math.sqrt(model.score(X, y)))\n\n# Make a prediction for Cyprus\nplt.scatter(X, y,color='b')\nplt.xlabel(\"Amount of Residual Sugar\")\nplt.ylabel(\"Quality Score\")\nplt.plot(X, model.predict(X),color='r') # outputs [[6.30165767]]\nplt.show()\nX_1 = [[4]]\nX_2 = [[8]]\nX_3 = [[12]]\nprint(\"The predicted score for a red wine with a residual sugar amount of 4 is %.2f\" % model.predict(X_1)[0][0])\nprint(\"The predicted score for a red wine with a residual sugar amount of 8 is %.2f\" % model.predict(X_2)[0][0])\nprint(\"The predicted score for a red wine with a residual sugar amount of 12 is %.2f\" % model.predict(X_3)[0][0])\n\n\n\n\n\nThe correlation of amount of residual sugar to wine quality is 0.01\nThe predicted score for a red wine with a residual sugar amount of 4 is 5.65\nThe predicted score for a red wine with a residual sugar amount of 8 is 5.68\nThe predicted score for a red wine with a residual sugar amount of 12 is 5.71\n\n\n\n\n\nObserving the results of our regression analysis, we see that it is extremely difficult to predict the quality score of a red wine based on the amount of residual sugar within. We once again show this in 3 ways. First, the correlation value is almost 0. Secondly, the line of regression is almost horizontal. Thirdly, predicting the quality score of different red wines with different amounts of residual sugar provides almost identical scoring. With these 3 things in mind, we can infer that the amount of residual sugar is not a good metric to infer the quality score of a red wine.\nAnd that concludes this post regarding (linear) regression. If you would like more information on linear regression, or regression in general, check out these sources that I also utilized in the making of this blog post: Harvard Business Review - A Refresher on Regression Analysis (https://hbr.org/2015/11/a-refresher-on-regression-analysis) IMSL - What Is a Regression Model? (https://www.imsl.com/blog/what-is-regression-model)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MachineLearningBlog",
    "section": "",
    "text": "Clustering\n\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2023\n\n\nMichael Alex Kyer\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\n\n\n\n\n\nOct 29, 2023\n\n\nMichael Alex Kyer\n\n\n\n\n\n\n  \n\n\n\n\nLinear Regression Blogpost\n\n\n\n\n\n\n\n\n\n\n\n\nOct 29, 2023\n\n\nMichael Alex Kyer\n\n\n\n\n\n\n  \n\n\n\n\nProbability theory and random variables\n\n\n\n\n\n\n\n\n\n\n\n\nOct 29, 2023\n\n\nMichael Alex Kyer\n\n\n\n\n\n\n  \n\n\n\n\nError Analysis\n\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2023\n\n\nMichael Alex Kyer\n\n\n\n\n\n\nNo matching items"
  }
]