[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/linear-regression/index.html",
    "href": "posts/linear-regression/index.html",
    "title": "Linear Regression Blogpost",
    "section": "",
    "text": "What is Regression\n\nSimply put, regression is a method used in data analysis which compares one or more independent variables to a single dependent variable. To further define this term, let’s use the analogy of a final exam for a difficult test. The dependent variable here will be the score of the exam, and there can be a multitude of factors, or independent variables, that play into what score you get. The time spent studying for the exam, the number of lectures attended, the amount of sleep the night before, what was ate the day of the exam, other stressful events currently going on, and so on. Now assume you have data on all of these factors for a multitude of students. Some questions you might have include Which factors will affect the final score the most? Will some factors not affect the score at all? We can use regression to answer these questions. A regression model will create a function that predicts the result of the dependent variable from the value of the independent variable.\n\nHow to Use a Regression Model\n\nTo create the aforementioned function, a plot is created with the y values mapping to the dependent variable, and the x values mapping to the independent variable. In our final exam analogy, we would have a multitude of plots. Assuming a sample of 100 students, one plot would have 100 points mapping the hours spent studying (x value) to their score on the final exam (y value). Another could map the number of lectures attended to the final exam score. A third could map the calories eaten before the exam to the final exam score. These plots could go on and on. For each plot created, we can use regression to generate the aforementioned function, and this function can be used to determine a correlation between each independent variable, and the dependent variable. It is also important to note that there are multiple different kinds of regression including linear, multiple, non-linear, and many more. We will use a linear regression model as it is the most simple, and the most frequently used. That being said, a multiple regression model could be used for our previous example to simultaneously analyze a multitude of independent variables on their effect on the final exam score.\n\nLinear Regression Example: An Analysis of Red Wine Quality\n\nIn this next example, we will use code to perform a regression analysis on data provided by UC Irvine on how a variety of factors affect red wine quality.\n\n\nCode\nimport sys\n!{sys.executable} -m pip install ucimlrepo\nimport ucimlrepo\nfrom ucimlrepo import fetch_ucirepo \nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.linear_model import LinearRegression\n\n\nRequirement already satisfied: ucimlrepo in /Users/alexkyer/anaconda3/lib/python3.11/site-packages (0.0.1)\n\n\n\n\nCode\nwine_quality = pd.read_csv(\"winequality-red.csv\")\n\nalcohol = []\ntarget = []\nfor index, row in wine_quality.iterrows():\n    values = row.values[0].split(';')\n    target.append(float(values[-1]))\n    alcohol.append(float(values[-2]))\n\nd = {'Alcohol Percentage': alcohol, 'Quality Score': target}\nwine_dataframe = pd.DataFrame(data=d)\n\nX = wine_dataframe[[\"Alcohol Percentage\"]].values\ny = wine_dataframe[[\"Quality Score\"]].values\n\n# Visualize the data\nwine_dataframe.plot(kind='scatter', grid=True,\n             x=\"Alcohol Percentage\", y=\"Quality Score\")\nplt.axis([8, 15, 2, 10])\nplt.show()\n\n\n\n\n\nWith the above plot, we observe a graph representation of the wine dataset. We have plotted a point for each entry in the provided CSV file with an x-value equal to the alchohol percentage, and a y-value equal to the quality score. Based on observing the visual graph alone, we can begin to make assumptions regarding the data, but we can go further by utilizing regression.\n\n\nCode\nmodel = LinearRegression()\n\nmodel.fit(X, y)\n\nprint(\"The correlation of alcohol percent to wine quality is %.2f\" % math.sqrt(model.score(X, y)))\n\n# Make a prediction for Cyprus\nplt.scatter(X, y,color='b')\nplt.xlabel(\"Alcohol Percentage\")\nplt.ylabel(\"Quality Score\")\nplt.plot(X, model.predict(X),color='r') # outputs [[6.30165767]]\nplt.show()\nX_1 = [[8.5]]\nX_2 = [[11.1]]\nX_3 = [[14.7]]\nprint(\"The predicted score for a red wine with an alcohol percentage of 8.5 is %.2f\" % model.predict(X_1)[0][0])\nprint(\"The predicted score for a red wine with an alcohol percentage of 11.1 is %.2f\" % model.predict(X_2)[0][0])\nprint(\"The predicted score for a red wine with an alcohol percentage of 14.7 is %.2f\" % model.predict(X_3)[0][0])\n\n\nThe correlation of alcohol percent to wine quality is 0.48\nThe predicted score for a red wine with an alcohol percentage of 8.5 is 4.94\nThe predicted score for a red wine with an alcohol percentage of 11.1 is 5.88\nThe predicted score for a red wine with an alcohol percentage of 14.7 is 7.18\n\n\n\n\n\nUtilizing the sklearn library, we can gather additional information regarding the relationship between alcohol percentage and the quality score given to the wine. First, we observe the correlation of these variables is approximately 0.48. This means that there is a moderate, positive correlation between these values. This fact is further shown when plotting a regression line on top of the graph. We see that this line is at a positive diagonal, and here we can infer that a red wine with a higher alcohol percentage will likely have a higher score than a red wine with a lower alcohol percentage. This fact is further illustrated by utilizing the sklearn predict method. This method will use the existing data and the regression line to predict the y-value of an x-value added to the set. Here, we have predicted the quality score for red wines with an alcohol percentage of 8.5, 11.1, and 14.7. The predicted scores increased as the theoretical alcohol percetage values increased, and this is yet another way to visualize the relationship between alcohol percentage and the quality score of red wine.\nWith this in mind, let’s repeat this process with another independent variable provided in the data set, the amount of residual sugar. Repeating this process can provide us with a correlation between the amount of residual sugar and the quality score.\n\n\nCode\nresidual_sugar = []\nfor index, row in wine_quality.iterrows():\n    values = row.values[0].split(';')\n    residual_sugar.append(float(values[3]))\n\nd = {'Amount of Residual Sugar': residual_sugar, 'Quality Score': target}\nwine_dataframe = pd.DataFrame(data=d)\n\nX = wine_dataframe[[\"Amount of Residual Sugar\"]].values\ny = wine_dataframe[[\"Quality Score\"]].values\n\n# Visualize the data\nwine_dataframe.plot(kind='scatter', grid=True,\n             x=\"Amount of Residual Sugar\", y=\"Quality Score\")\nplt.axis([0, 17.5, 2, 10])\nplt.show()\n\nmodel = LinearRegression()\n\nmodel.fit(X, y)\n\nprint(\"The correlation of amount of residual sugar to wine quality is %.2f\" % math.sqrt(model.score(X, y)))\n\n# Make a prediction for Cyprus\nplt.scatter(X, y,color='b')\nplt.xlabel(\"Amount of Residual Sugar\")\nplt.ylabel(\"Quality Score\")\nplt.plot(X, model.predict(X),color='r') # outputs [[6.30165767]]\nplt.show()\nX_1 = [[4]]\nX_2 = [[8]]\nX_3 = [[12]]\nprint(\"The predicted score for a red wine with a residual sugar amount of 4 is %.2f\" % model.predict(X_1)[0][0])\nprint(\"The predicted score for a red wine with a residual sugar amount of 8 is %.2f\" % model.predict(X_2)[0][0])\nprint(\"The predicted score for a red wine with a residual sugar amount of 12 is %.2f\" % model.predict(X_3)[0][0])\n\n\n\n\n\nThe correlation of amount of residual sugar to wine quality is 0.01\nThe predicted score for a red wine with a residual sugar amount of 4 is 5.65\nThe predicted score for a red wine with a residual sugar amount of 8 is 5.68\nThe predicted score for a red wine with a residual sugar amount of 12 is 5.71\n\n\n\n\n\nObserving the results of our regression analysis, we see that it is extremely difficult to predict the quality score of a red wine based on the amount of residual sugar within. We once again show this in 3 ways. First, the correlation value is almost 0. Secondly, the line of regression is almost horizontal. Thirdly, predicting the quality score of different red wines with different amounts of residual sugar provides almost identical scoring. With these 3 things in mind, we can infer that the amount of residual sugar is not a good metric to infer the quality score of a red wine.\nAnd that concludes this post regarding (linear) regression. If you would like more information on linear regression, or regression in general, check out these sources that I also utilized in the making of this blog post: Harvard Business Review - A Refresher on Regression Analysis (https://hbr.org/2015/11/a-refresher-on-regression-analysis) IMSL - What Is a Regression Model? (https://www.imsl.com/blog/what-is-regression-model)"
  },
  {
    "objectID": "posts/ensemble-models/EnsembleModels.html",
    "href": "posts/ensemble-models/EnsembleModels.html",
    "title": "Ensemble Models",
    "section": "",
    "text": "What is an Ensemble Model\nSimply put, ensemble models are the culmination of multiple other models with the goal of strengthening the overall prediction capabilities. A lot of issues can arise when only utilizing one model. We’ll look into what those issues are later, but for now let’s use an analogy of a visit to the doctor to describe the basics of ensemble models. Let’s say that you are suffering from an undiagnosed ailment, and you would like to figure out what it is. To do so, you’ll take a trip to your doctor’s office, and receive a diagnosis. There they tell you that it is probably nothing. However, you are not convinced of this diagnosis, so you travel to another doctor for a second opinion. This one diagnoses you with a life threatening condition, and you need immediate treatment. Now you have two very different diagnoses, so you might seek out a third, fourth, maybe even fifth opinion to really get to the bottom of what this issue is. In the end, you’ll follow the procedure and diagnosis that most doctors you saw agreed upon. Machine learning is very much similar with algorithms predicting an outcome like the doctors were providing a diagnosis. With a single model, you can receive inaccurate results as a doctor provided an incorrect diagnosis. Ensemble models solve this issue just like gathering the opinion of other doctors can provide a more accurate diagnosis. Firstly, an ensemble model combine a multitude of individual models. Then a single input is passed through each individual model, so a collection of outputs are produced. These outputs are the predictions, and the ensemble model can select its overall prediction from the collection. Most likely, the selected prediction will be either the most frequently appearing prediction from the individual models, or it is some average value of the predictions. Now, we can see that the utilization of ensemble models solves the issues of single models by potentially increasing accurracy while lowering the error.\n\nWhat Are Decision Trees\nFor each of the models discussed here, they will create a multitude of smaller decision tree models. One of the more common types of decision tree is a binary decision tree. This is a tree structure where each inner node is a yes/no decision to make, and each node has two branches, one yes and one no. An input value is passed in and the decisions are made one by one down the tree until a leaf node is eventually reached. These leaf nodes contain the final prediction that the model will make for the given input.\n\n\n\nCommonly Used Ensemble Techniques\nBoth of the techniques discussed can be applied to regression or classification trees with subtle differences in each. Using regression trees will oftentimes have the selected prediction being the average of the individual models. On the other hand, the ensemble will take the most frequent occurrence if classification trees are used. ### Bagging The first of the two techniques we will discuss is known as bagging or bootstrap aggregation. Here there is one training set, and a multitude of tree models will be constructed from that training set. The difference between the models is that each one is trained with a different subset of the training set. Each subset will be equal in size, and the values in each will reflect the values of the training set. However, to make each subset unique and equal in size, it will allow for repeated entries. The result is a set of unique subsets which train a set of tree models of the same size. These models then each individually predict the outcome of an input, and either the average or most common prediction is chosen. ### Random Forest A random forest ensemble model is very similar to the bagging ensemble model, but there is one key difference. Each node can only be split on a random selection of predictors. With most datasets, certain predictors are more significant than others, and this approach means that the more significant predictors might be found lower on the tree. Now a collection of these models will produce very different results. With these different models, the same approach of taking the average or most common prediction of these models can then be used.\n\n\nRandom Forest Ensemble Model: An Analysis of Billionaires Amongst Billionaires\nIn this example, we are going to analyze a dataset of billionaires and their various statistics including age, whether they are self-made, and a variety of statistics regarding their country of citizenship. Our goal is to see if we can use this data to create a model that accurately predicts whether or not a given billionaire is in the upper half of other billionaires in terms of their net worth. ### Importing Libraries\n\n\nCode\nimport sys\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split\nfrom scipy.stats import randint\n\n\n\nPreprocessing Data\nFor this next step, we will have to do some preprocessing of the data in order to turn each of the values of the table into either a binary 0 or 1. 0s will correspond to a no answer, and 1s will correspond to a yes answer. First we will drop each of the tables that are purely textual including their name, country, city, title, gender, and so on. There is likely very useful information within the dropped data, but for now we will solely focus on the numerical inputs. Then we will utilize a binary classifier to change each entry to either a 0 or a 1. The classifier will be if the entry is less than or equal to the median for 0, or greater than the median for 1. After this step we will be able to begin our model creation, training, and prediction steps.\n\n\nCode\nbillionaire_data = pd.read_csv(\"billionaire.csv\")\n\nbillionaire_data = billionaire_data.drop('finalWorth', axis=1)\nbillionaire_data = billionaire_data.drop('category', axis=1)\nbillionaire_data = billionaire_data.drop('personName', axis=1)\nbillionaire_data = billionaire_data.drop('country', axis=1)\nbillionaire_data = billionaire_data.drop('city', axis=1)\nbillionaire_data = billionaire_data.drop('industries', axis=1)\nbillionaire_data = billionaire_data.drop('source', axis=1)\nbillionaire_data = billionaire_data.drop('countryOfCitizenship', axis=1)\nbillionaire_data = billionaire_data.drop('organization', axis=1)\nbillionaire_data = billionaire_data.drop('lastName', axis=1)\nbillionaire_data = billionaire_data.drop('firstName', axis=1)\nbillionaire_data = billionaire_data.drop('title', axis=1)\nbillionaire_data = billionaire_data.drop('date', axis=1)\nbillionaire_data = billionaire_data.drop('state', axis=1)\nbillionaire_data = billionaire_data.drop('residenceStateRegion', axis=1)\nbillionaire_data = billionaire_data.drop('birthDate', axis=1)\nbillionaire_data = billionaire_data.drop('status', axis=1)\nbillionaire_data = billionaire_data.drop('gender', axis=1)\nbillionaire_data = billionaire_data.drop('gdp_country', axis=1)\n\nbillionaire_data.dropna(inplace=True)\n        \ndef makeBinary(value, median):\n    if int(value) &lt;= median:\n        return 0\n    else:\n        return 1\n\ntoBinaryColumns = ['rank', 'cpi_country', 'cpi_change_country', 'gross_tertiary_education_enrollment', 'gross_primary_education_enrollment_country',\n                  'life_expectancy_country', 'tax_revenue_country_country', 'total_tax_rate_country', 'population_country', 'latitude_country', \n                  'longitude_country']\n\nfor columnName in toBinaryColumns:\n    billionaire_data[columnName] = billionaire_data[columnName].apply(makeBinary, median=billionaire_data[columnName].median())\n    \nbillionaire_data['selfMade'] = billionaire_data['selfMade'].map({False:0,True:1})\n\nX = billionaire_data.drop('rank', axis=1)\ny = billionaire_data['rank']\n\n\n\n\nInitial Fit and Prediction\nHere we will split our data into a train and test set. We will first train our model so that it can perform a series of predictions. We will then use the trained model to predict the answers to our test set, and compare them to their true result.\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nrf = RandomForestClassifier()\nrf.fit(X_train, y_train)\n\ny_pred = rf.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\n\nAccuracy: 0.5770833333333333\n\n\n\n\nAttempt to Improve our Accuracy\nNext we will attempt to improve our accuracy by passing in random sets of hyperparameters, and calculates the score for each. The set with the highest score is then saved to be used to make predictions later.\n\n\nCode\nparam_dist = {'n_estimators': randint(50,500),\n              'max_depth': randint(1,20)}\n\nrf = RandomForestClassifier()\n\nrand_search = RandomizedSearchCV(rf, param_distributions = param_dist, n_iter=5, cv=5)\n\nrand_search.fit(X_train, y_train)\n\nbest_rf = rand_search.best_estimator_\n\nprint('Best hyperparameters:',  rand_search.best_params_)\n\n\nBest hyperparameters: {'max_depth': 3, 'n_estimators': 364}\n\n\n\n\nUsing the Best Hyperparameters to Make Predictions\nWith the set of the best hyperparameters, we will again test the model against our set aside testing data, and compare the predicted results to their actual values. We will first show our results in the form of a confusion matrix. Afterwards, we will use a bar graph to show the most important predictors in determining the correct value.\n\n\nCode\n# Generate predictions with the best model\ny_pred = best_rf.predict(X_test)\n\n# Create the confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\nConfusionMatrixDisplay(confusion_matrix=cm).plot();\n\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\n\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\n\n\nAccuracy: 0.6395833333333333\nPrecision: 0.6293103448275862\nRecall: 0.6266094420600858\n\n\n\n\n\n\n\nCode\nfeature_importances = pd.Series(best_rf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n\nfeature_importances.plot.bar();\n\n\n\n\n\n\n\nInterpreting the Data\nObserving the data above, and running the creation and prediction of the model a few times, we can see that there is improvement by tuning the models, if only slightly. When tuning our model to the optimal hyperparameters, we see a roughly 2% increase in accuracy for predicting whether or not a billionaire was in the upper echelon of other billionaires. As it is a binary prediction, a naive dummy classifier could predict either a yes or no everytime and receive a 50% success rate. Our results are better, but only slightly.\nWe can observe the confusion matrix above to determine a multitude of values, but we will discuss more in depth what these values mean in a different blog post. For now we will just define the values listed above and how they can be interpreted.\nAccuracy: The number of predictions that were correct Positive Precision: The number of true positives compared to the number of all predicted positives Positive Recall: The number of true positives compared to the number of all actual positives\nLooking at the most important predictors, the ones frequently labelled as the most important, are those centered around age. Birthyear, age, birthday, are always among the top predictors. This makes sense as those who are older have more time to accrue wealth than those who are younger.\n\n\n\nWrap Up\nThat concludes this blogpost on ensemble models. If you would like to continue learning about ensemble models in general, other models, or either of the models discussed here, I have provided a list of articles used in the creation of this blogpost\nbuiltIn - Ensemble Models: What Are They and When Should You Use Them? (https://builtin.com/machine-learning/ensemble-model) dataiku’s Blog - Tree-Based Models: How They Work (In Plain English!) (https://blog.dataiku.com/tree-based-models-how-they-work-in-plain-english#:~:text=Ensemble%20models%20can%20be%20used,non%2Dlinear%20relationships%20quite%20well.) datacamp - Random Forest Classification with Scikit-Learn (https://www.datacamp.com/tutorial/random-forests-classifier-pythonRandom Forest Classification with Scikit-Learn)\nBillionaires Dataset Used - Billionaires Statistics Dataset (2023) by Nidula Elgiriyewithana (https://www.kaggle.com/datasets/nelgiriyewithana/billionaires-statistics-dataset/data)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MachineLearningBlog",
    "section": "",
    "text": "Ensemble Models\n\n\n\n\n\n\n\n\n\n\n\n\nSep 29, 2023\n\n\nMichael Alex Kyer\n\n\n\n\n\n\n  \n\n\n\n\nLinear Regression Blogpost\n\n\n\n\n\n\n\n\n\n\n\n\nSep 26, 2023\n\n\nMichael Alex Kyer\n\n\n\n\n\n\nNo matching items"
  }
]